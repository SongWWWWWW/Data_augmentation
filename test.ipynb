{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"./vast/raw_train_all_onecol.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "text = '[CLS]'\n",
    "sample_texts = [\"I love this movie!\", \"I hate this movie.\"]\n",
    "sample_stance = [\"movie\",\"movie\"]\n",
    "labels = [1, 0]  # 1: positive, 0: negative\n",
    "\n",
    "# Encoding the texts\n",
    "encodings = tokenizer(sample_texts, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "print(encodings[\"input_ids\"])\n",
    "encodings_stance = tokenizer(sample_stance,truncation=False, padding=False,return_tensors=\"pt\")\n",
    "print(encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "def generate_answer(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=\"\"\n",
    "        )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"o1-preview-2024-09-12\",\n",
    "        stream=False,\n",
    "    )\n",
    "    chat_completion = chat_completion.choices[0].message.content\n",
    "    return chat_completion\n",
    "\n",
    "generate_answer(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.roberta_mlp import RoBERTa_MLP\n",
    "import torch\n",
    "\n",
    "# 初始化模型架构\n",
    "model = RoBERTa_MLP(num_labels=3)\n",
    "\n",
    "# 加载保存的权重\n",
    "model_path = \"./scripts/results/checkpoint-106/\"\n",
    "state_dict = torch.load(f\"{model_path}/model.safetensors\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "from scripts.roberta_mlp import RoBERTa_MLP\n",
    "\n",
    "# 初始化模型\n",
    "model = RoBERTa_MLP(num_labels=3)\n",
    "\n",
    "# 加载 .safetensors 权重\n",
    "model_path = \"./scripts/results/checkpoint-106/model.safetensors\"\n",
    "state_dict = load_file(model_path)\n",
    "\n",
    "# 将加载的权重加载到模型中\n",
    "model.load_state_dict(state_dict)\n",
    "model.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# path = \"./data/wrong_test_data_424.json\"\n",
    "# with open(path,\"r\") as f:\n",
    "#     data = json.load(f)\n",
    "# print(len(data))\n",
    "# data\n",
    "path = \"\"\"vast/raw_{}_all_onecol.csv\"\"\"\n",
    "data = pd.read_csv(path.format(\"train\"))\n",
    "# print(len(data))\n",
    "# str(data[:1])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import toml\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List\n",
    "import copy\n",
    "args = toml.load(\"key.toml\")\n",
    "path = \"./data/wrong_test_data_424.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "openai.api_key = args[\"gpt-cot\"][\"openai_api_key\"]\n",
    "batch = 30\n",
    "def generate_answer(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=openai.api_key\n",
    "        )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        stream=False,\n",
    "        temperature=0,\n",
    "    )\n",
    "    chat_completion = chat_completion.choices[0].message.content\n",
    "    return chat_completion\n",
    "def prepare_data(data:List[dict],num,batch):\n",
    "    d = copy.deepcopy(data[num*batch:(num+1)*batch])\n",
    "    for i in d:\n",
    "        try:\n",
    "            del i[\"index\"]\n",
    "        except Exception as e:\n",
    "            print(\"error\")\n",
    "    return str(d)\n",
    "prompt = \"\"\"\n",
    "You are an expert in multi-level weakness detection and augmentation. \n",
    "I will provide you with {batch} pieces of data where other models have given incorrect answers, \n",
    "related to stance detection. The label mapping is as follows: {{ \"FAVOR\": 0, \"NONE\": 1, \"AGAINST\": 2 }}\n",
    "Please identify the similarities in the data.Data as follow.\n",
    "{data}\n",
    "please identify the similarities in the data.\n",
    "\"\"\"\n",
    "# print(prepare_data(data,1,batch))\n",
    "# answer = generate_answer(prompt.format(batch=batch,data=prepare_data(data,batch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(answer)\n",
    "answer = []\n",
    "for i in range(len(data)//batch):\n",
    "    response = generate_answer(prompt.format(batch=batch,data=prepare_data(data,i,batch)))\n",
    "    answer.append(response)\n",
    "    print(\"-\"*20)\n",
    "    print(f\"index : from {i*batch} to {(i+1)*batch}\")\n",
    "    print(response)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/get_response_mistake.json\",\"w\") as f:\n",
    "    json.dump(answer,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You will be provided with five responses regarding stance detection. \n",
    "Please identify the common points in the following responses.\n",
    "{responses}\n",
    "\"\"\"\n",
    "answer_batch = []\n",
    "for i in range(len(answer)//5+1):\n",
    "    x = generate_answer(prompt.format(responses = str(answer[i*5:(i+1)*5])))\n",
    "    answer_batch.append(x)\n",
    "    print(\"-\"*20)\n",
    "    print(f\"index : from {i*5} to {(i+1)*5}\")\n",
    "    print(x)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You will be provided with five responses regarding stance detection. \n",
    "Please identify the common points in the following responses.\n",
    "{responses}\n",
    "\"\"\"\n",
    "print(generate_answer(prompt.format(responses=answer_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert in multi-level weakness detection and augmentation. \n",
    "I will provide you with {batch} pieces of data where other models have given incorrect answers \n",
    "related to stance detection. The label mapping is as follows: {{\"FAVOR\": 0, \"NONE\": 1, \"AGAINST\": 2}}.\n",
    "Please identify the similarities in the data and explain why these similarities arise. The data is as follows:\n",
    "{data}\n",
    "Please identify the similarities in the data.\n",
    "\"\"\"\n",
    "# print(prepare_data(data,0,batch))\n",
    "answer = generate_answer(prompt.format(batch=batch,data=prepare_data(data,0,batch)))\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert instructor specializing in educational diagnostics and performance analysis. Your task is to assess student performance on {task_name}.\n",
    "\n",
    "Task Context:\n",
    "{task_description}\n",
    "\n",
    "I will present you with {batch} instances where the student provided incorrect responses. The classification labels are defined as follows: {{\"FAVOR\": 0, \"NONE\": 1, \"AGAINST\": 2}}.\n",
    "\n",
    "Please conduct a thorough analysis of the student's systematic errors and learning gaps. For each identified weakness, provide:\n",
    "\n",
    "1. A detailed description of the error pattern\n",
    "2. Potential underlying causes\n",
    "3. Specific examples illustrating the weakness\n",
    "4. Suggestions for generating targeted practice materials\n",
    "5. Based on each weaknesses you have identified, please generate 100 similar sets of questions.\n",
    "\n",
    "Format your analysis using XML tags, with each distinct weakness enclosed in <Weakness i></Weakness i> tags, \n",
    "where i represents the sequential number of the weakness. And use <Error Pattern> <PotantialCauses>, <Example>, <Sugestion>, <TestSet> to detail each weakness. \n",
    "The <PotantialCauses> should directly indicate what kinds of text or semantic information could cause such failure.\n",
    "The <TestSet> should include {test_num} questions, each enclosed in <test i>, where 'i' is the sequential number of the test set. \n",
    "The <text>, <target>, and <ground_truth> elements should be within each <test i>.\n",
    "\n",
    "Your analysis should be comprehensive enough to guide the creation of tailored educational materials that directly address these learning gaps.\n",
    "\n",
    "The data are as follows:\n",
    "{data}\n",
    "\"\"\"\n",
    "test_num = 100\n",
    "\n",
    "task_name = \"stance dection\"\n",
    "\n",
    "task_description = \"Sentiment classification of comments on tweets.\"\n",
    "\n",
    "batch = 30\n",
    "\n",
    "# print(prompt.format(task_name = task_name, task_description = task_description, batch = batch, data=prepare_data(data,0,batch)))\n",
    "answer = generate_answer(prompt.format(test_num=test_num,task_name = task_name, task_description = task_description, batch=batch, data=prepare_data(data,0,batch)))\n",
    "print(answer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "# 解析XML数据\n",
    "root = ET.fromstring(answer.strip(\"```xml\").strip(\"```\"))\n",
    "\n",
    "# 定义一个函数将XML转换为JSON格式\n",
    "def xml_to_json(element):\n",
    "    json_data = {}\n",
    "    for child in element:\n",
    "        # 获取子元素的标签名作为key，子元素内容作为value\n",
    "        json_data[child.tag] = child.text.strip() if len(child) == 0 else xml_to_json(child)\n",
    "    return json_data\n",
    "\n",
    "# 提取根元素\n",
    "json_result = xml_to_json(root)\n",
    "\n",
    "# 打印JSON格式的结果\n",
    "print(json.dumps(json_result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakness = \"\"\"\n",
    "<Weakness1>\n",
    "    <ErrorPattern>The student frequently misclassifies comments that express a nuanced or complex opinion as being against the topic, rather than recognizing them as neutral or favoring the topic.</ErrorPattern>\n",
    "    <PotentialCauses>The student may struggle with identifying subtle sentiment cues in the text, particularly when the language used is critical but not outright negative. They might also have difficulty distinguishing between sarcasm or critique and genuine opposition.</PotentialCauses>\n",
    "    <Example>The comment about the need for intercity passenger rail discusses costs and safety but is misclassified as \"NONE\" instead of \"FAVOR\".</Example>\n",
    "    <Suggestion>Practice materials should include examples of nuanced opinions, asking students to identify whether the sentiment is favoring, neutral, or against the topic. Include discussions on sarcasm and critique in sentiment analysis.</Suggestion>\n",
    "</Weakness1>\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an expert at generating data based on weaknesses and causes. \n",
    "Given the data with tags like ErrorPattern, PotentialCauses, Example, and Suggestion, \n",
    "generate {num_examples} examples using the following format:\n",
    "\n",
    "<Example i>\n",
    "    <text>Text content</text>\n",
    "    <target>Entity in the text</target>\n",
    "    <truth_label>Truth label (correct classification)</truth_label>\n",
    "</Example i>\n",
    "\n",
    "Provide the generated examples based on the provided {weakness} data.\n",
    "\"\"\"\n",
    "num_examples = 100\n",
    "answer_weakness = generate_answer(prompt.format(num_examples=num_examples,weakness=weakness))\n",
    "print(answer_weakness)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 单条数据生成testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert instructor specializing in educational diagnostics and performance analysis. Your task is to assess student performance on {task_name}.\n",
    "\n",
    "Task Context:\n",
    "{task_description}\n",
    "\n",
    "I will present you with an instance where the student provided incorrect responses. The classification labels are defined as follows: {{\"FAVOR\": 0, \"NONE\": 1, \"AGAINST\": 2}}.\n",
    "\n",
    "Please conduct a thorough analysis of the student's systematic errors and learning gaps. For each identified weakness, provide:\n",
    "\n",
    "1. A detailed description of the error pattern\n",
    "2. Potential underlying causes\n",
    "3. Based on each weaknesses you have identified, please generate 100 similar sets of questions.\n",
    "\n",
    "Format your analysis using XML tags, with each distinct weakness enclosed in <Weakness i></Weakness i> tags, \n",
    "where i represents the sequential number of the weakness. And use <Error Pattern> <PotantialCauses>, <TestSet> to detail each weakness. \n",
    "The <PotantialCauses> should directly indicate what kinds of text or semantic information could cause such failure.\n",
    "The <TestSet> should include {test_num} questions, each enclosed in <test i>, where 'i' is the sequential number of the test set. \n",
    "The <text>, <target>(Entity in the text), and <ground_truth>(Truth label) elements should be within each <test i>.\n",
    "Ensure that the length of the text varies (the length exceeds 100)and that the diversity of the text is also guaranteed (you can choose materials from different fields),\n",
    "and the text should be in line with <Weakness i>.\n",
    "\n",
    "Your analysis should be comprehensive enough to guide the creation of tailored educational materials that directly address these learning gaps.\n",
    "\n",
    "The data are as follows:\n",
    "{data}\n",
    "\"\"\"\n",
    "batch = 1\n",
    "test_num = 100\n",
    "\n",
    "task_name = \"stance detection\"\n",
    "\n",
    "task_description = \"Stance detection aims to identify the authors' attitudes or positions [FAVOR, NONE, AGAINST] towards a specific target such as an entity, a topic.\"\n",
    "\n",
    "\n",
    "\n",
    "print(prompt.format(test_num=test_num,task_name = task_name, task_description = task_description, data=prepare_data(data,0,batch)))\n",
    "answer = generate_answer(prompt.format(test_num=test_num,task_name = task_name, task_description = task_description, data=prepare_data(data,0,batch)))\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import openai\n",
    "import toml\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List\n",
    "import copy\n",
    "args = toml.load(\"key.toml\")\n",
    "path = \"./data/wrong_test_data_424.json\"\n",
    "\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"len(data): \", len(data))\n",
    "\n",
    "openai.api_key = args[\"gpt-cot\"][\"openai_api_key\"]\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=openai.api_key\n",
    "        )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        stream=False,\n",
    "        temperature=0,\n",
    "    )\n",
    "    chat_completion = chat_completion.choices[0].message.content\n",
    "    return chat_completion\n",
    "def prepare_data(data:List[dict],num):\n",
    "    d = copy.deepcopy(data[num])\n",
    "    # for i in d:\n",
    "    #     try:\n",
    "    #         del i[\"index\"]\n",
    "    #     except Exception as e:\n",
    "    #         print(\"error\",e)\n",
    "    del d[\"index\"]\n",
    "    return str(d)\n",
    "def get_answer(data,num_data:int):    \n",
    "    prompt = \"\"\"\n",
    "    You are an expert instructor specializing in educational diagnostics and teaching. Your task is to assess student performance on {task_name} \n",
    "    and provide the training instances to teach the student to become more expert.\n",
    "\n",
    "    Task Context:\n",
    "    {task_description}\n",
    "\n",
    "    I will present you with an instance where the student provided incorrect responses. The classification labels are defined as follows: {{\"FAVOR\": 0, \"NONE\": 1, \"AGAINST\": 2}}.\n",
    "\n",
    "    Please consider the potential weakness of the student considering the incorrect response and provide a training instances to teach it to be a more expert problem solver. \n",
    "\n",
    "    Format your training instances using XML tags.\n",
    "    The <TrainingSet> should include {training_num} questions, each enclosed in <train i>, where 'i' is the sequential number of the training set. \n",
    "    The <text>, <target>(an entity in the text), and <ground_truth>(Truth label) elements should be within each <train i>.\n",
    "\n",
    "    Ensure that the length of the text varies (the length exceeds 100) and that the diversity of the text is also guaranteed \n",
    "    (you can choose materials from different fields or different tones like diverse users on the website).\n",
    "\n",
    "    The data are as follows:\n",
    "    {data}\n",
    "    \"\"\"\n",
    "\n",
    "    batch = 1\n",
    "    training_num = 10\n",
    "\n",
    "    task_name = \"stance detection\"\n",
    "\n",
    "    task_description = \"Stance detection aims to identify the authors' attitudes or positions [FAVOR, NONE, AGAINST] towards a specific target such as an entity, a topic.\"\n",
    "\n",
    "    # print(prompt.format(training_num=training_num, task_name = task_name, task_description = task_description, data=prepare_data(data,num_data)))\n",
    "    answer = generate_answer(prompt.format(training_num=training_num, task_name = task_name, task_description = task_description, data=prepare_data(data,num_data)))\n",
    "    # print(answer)\n",
    "    return answer\n",
    "\n",
    "def parse(answer:str):\n",
    "    pattern = r\"```xml(.*?)```\"\n",
    "    match = re.search(pattern, answer, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1).strip()\n",
    "        print(\"Extracted Content:\")\n",
    "        print(extracted_content[:20],\".....\")\n",
    "    else:\n",
    "        print(\"No content found between ```xml and ```\")\n",
    "    try:\n",
    "        root = ET.fromstring(extracted_content)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", answer[:40])\n",
    "        return [answer]\n",
    "    js = []\n",
    "    for i in root:\n",
    "        k = {}\n",
    "        for j in i:\n",
    "            k[j.tag] = j.text\n",
    "        js.append(k)\n",
    "    print(\"parse successfully: len(parse_json) = \",len(js))\n",
    "    return js\n",
    "result = []\n",
    "complete_num = 0\n",
    "for i in range(len(data)):\n",
    "    print(\"-\"*30)\n",
    "    print(f\"index: {i}, complete_num: {complete_num}\")\n",
    "    d = get_answer(data,i)\n",
    "    if len(d) > 3:\n",
    "        complete_num += 1\n",
    "    result.append(parse(d))\n",
    "\n",
    "with open(\"./data/all_error_instances_weakness_generate_train_dataset.json\",\"w\") as f:\n",
    "    json.dump(result,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "b = [1]\n",
    "c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "pattern = r\"```xml(.*?)```\"\n",
    "match = re.search(pattern, answer, re.DOTALL)\n",
    "if match:\n",
    "    extracted_content = match.group(1).strip()\n",
    "    print(\"Extracted Content:\")\n",
    "    print(extracted_content[:14],\".....\")\n",
    "else:\n",
    "    print(\"No content found between ```xml and ```\")\n",
    "root = ET.fromstring(extracted_content)\n",
    "js = []\n",
    "for i in root:\n",
    "    # print(\"<\",i.tag,\">\")\n",
    "    k = {}\n",
    "    for j in i:\n",
    "        # print(\" <\",j.tag,\">\")\n",
    "        # print(j.text)\n",
    "        k[j.tag] = j.text\n",
    "    js.append(k)\n",
    "print(js)\n",
    "# with open(\"./data/sample_instance_weakness_generate_instances.json\",\"w\") as f:\n",
    "#     json.dump(js,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import openai\n",
    "import toml\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List\n",
    "import copy\n",
    "args = toml.load(\"key.toml\")\n",
    "\n",
    "with open(\"./data/all_error_instances_weakness_generate_train_dataset.json\",\"r\") as f:\n",
    "    response_data = json.load(f)\n",
    "\n",
    "path = \"./data/wrong_test_data_424.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "openai.api_key = args[\"gpt-cot\"][\"openai_api_key\"]\n",
    "\n",
    "def generate_answer(prompt):\n",
    "    client = openai.OpenAI(\n",
    "        api_key=openai.api_key\n",
    "        )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "        stream=False,\n",
    "        temperature=0,\n",
    "    )\n",
    "    chat_completion = chat_completion.choices[0].message.content\n",
    "    return chat_completion\n",
    "def prepare_data(data:List[dict],num):\n",
    "    d = copy.deepcopy(data[num])\n",
    "    # for i in d:\n",
    "    #     try:\n",
    "    #         del i[\"index\"]\n",
    "    #     except Exception as e:\n",
    "    #         print(\"error\",e)\n",
    "    del d[\"index\"]\n",
    "    return str(d)\n",
    "def get_answer(data,num_data:int):    \n",
    "    prompt = \"\"\"\n",
    "    You are an expert instructor specializing in educational diagnostics and teaching. Your task is to assess student performance on {task_name} \n",
    "    and provide the training instances to teach the student to become more expert.\n",
    "\n",
    "    Task Context:\n",
    "    {task_description}\n",
    "\n",
    "    I will present you with an instance where the student provided incorrect responses. The classification labels are defined as follows: {{\"FAVOR\": 0, \"NONE\": 1, \"AGAINST\": 2}}.\n",
    "\n",
    "    Please consider the potential weakness of the student considering the incorrect response and provide a training instances to teach it to be a more expert problem solver. \n",
    "\n",
    "    Format your training instances using XML tags.\n",
    "    The <TrainingSet> should include {training_num} questions, each enclosed in <train i>, where 'i' is the sequential number of the training set. \n",
    "    The <text>, <target>(an entity in the text), and <ground_truth>(Truth label) elements should be within each <train i>.\n",
    "\n",
    "    Ensure that the length of the text varies (the length exceeds 100) and that the diversity of the text is also guaranteed \n",
    "    (you can choose materials from different fields or different tones like diverse users on the website).\n",
    "\n",
    "    The data are as follows:\n",
    "    {data}\n",
    "    \"\"\"\n",
    "\n",
    "    batch = 1\n",
    "    training_num = 10\n",
    "\n",
    "    task_name = \"stance detection\"\n",
    "\n",
    "    task_description = \"Stance detection aims to identify the authors' attitudes or positions [FAVOR, NONE, AGAINST] towards a specific target such as an entity, a topic.\"\n",
    "\n",
    "    # print(prompt.format(training_num=training_num, task_name = task_name, task_description = task_description, data=prepare_data(data,num_data)))\n",
    "    answer = generate_answer(prompt.format(training_num=training_num, task_name = task_name, task_description = task_description, data=prepare_data(data,num_data)))\n",
    "    # print(answer)\n",
    "    return answer\n",
    "def parse(answer:str):\n",
    "    pattern = r\"```xml(.*?)```\"\n",
    "    match = re.search(pattern, answer, re.DOTALL)\n",
    "    if match:\n",
    "        extracted_content = match.group(1).strip()\n",
    "        print(\"Extracted Content:\")\n",
    "        print(extracted_content[:20],\".....\")\n",
    "    else:\n",
    "        print(\"No content found between ```xml and ```\")\n",
    "    try:\n",
    "        root = ET.fromstring(extracted_content)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", answer[:40])\n",
    "        return [answer]\n",
    "    js = []\n",
    "    for i in root:\n",
    "        k = {}\n",
    "        for j in i:\n",
    "            k[j.tag] = j.text\n",
    "        js.append(k)\n",
    "    print(\"parse successfully: len(parse_json) = \",len(js))\n",
    "    return js\n",
    "def repeat_generate(raw_data,index,num:int):\n",
    "    if num > 2:\n",
    "        print(f\"error : don't generate resolvable answer, index: {index}\")\n",
    "        return [index]\n",
    "    print(\"repeat num: \", num)\n",
    "    answer = get_answer(raw_data,index)\n",
    "    parse_result = parse(answer)\n",
    "    if len(parse_result) < 10:\n",
    "        return repeat_generate(raw_data,index,num+1)\n",
    "    return parse_result\n",
    "response_data_copy = copy.deepcopy(response_data)\n",
    "for index, (response, raw) in enumerate(zip(response_data, raw_data)):\n",
    "    if len(response) < 10:\n",
    "        response_data_copy[index] = repeat_generate(raw_data,index,0)\n",
    "\n",
    "with open(\"./data/all_error_instances_weakness_generate_train_dataset_2.json\",\"w\") as f:\n",
    "    json.dump(response_data_copy,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/all_error_instances_weakness_generate_train_dataset.json\",\"r\") as f:\n",
    "#     response_data = json.load(f)\n",
    "\n",
    "# for i in response_data:\n",
    "#     if len(i) < 10:\n",
    "#         print(i[0])\n",
    "# with open(\"./data/all_error_instances_weakness_generate_train_dataset_2.json\",\"w\") as f:\n",
    "#     json.dump(response_data_copy,f,indent=4)\n",
    "with open(\"./data/all_error_instances_weakness_generate_train_dataset_2.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)\n",
    "sum = 0\n",
    "l = []\n",
    "for i in data:\n",
    "    if len(i) == 10:\n",
    "        for j in i:\n",
    "            # print(j)\n",
    "            if j[\"ground_truth\"] != '0' and j[\"ground_truth\"] != '1' and j['ground_truth'] != '2' and j['ground_truth'] not in l:\n",
    "                print(j['ground_truth'])\n",
    "                l.append(j['ground_truth'])\n",
    "            # if list(j.keys()) != ['text', 'target', 'ground_truth']:\n",
    "            #     print(j.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index_response': 28, 'index_stance': 5, 'text': \"I'm not sure if I would cancel my online subscription to the Times if they didn't improve their comment section. I need to weigh the pros and cons.\", 'target': 'comment section', 'ground_truth': ''}\n",
      "{'index_response': 28, 'index_stance': 15, 'text': \"I'm not sure if the Times is censoring comments or not. I need more information to form an opinion.\", 'target': 'comment section', 'ground_truth': ''}\n",
      "{'index_response': 28, 'index_stance': 25, 'text': \"I'm not sure if the Times is unduly influencing public opinion or not. I need more information to form an opinion.\", 'target': 'comment section', 'ground_truth': ''}\n",
      "{'index_response': 45, 'index_stance': 15, 'text': \"I'm not sure what to make of public opinion polls. Sometimes they seem accurate, but other times they're way off.\", 'target': 'democracy', 'ground_truth': ''}\n",
      "{'index_response': 45, 'index_stance': 19, 'text': \"I'm not sure how much stock to put in public opinion polls. Sometimes they seem to reflect the views of the people, but other times they don't.\", 'target': 'democracy', 'ground_truth': ''}\n",
      "{'index_response': 204, 'index_stance': 15, 'text': \"I'm not sure I understand the concept of mutual acceptance and acknowledgement. Can someone explain it to me?\", 'target': 'mutual acceptance', 'ground_truth': ''}\n",
      "{'index_response': 204, 'index_stance': 25, 'text': \"I'm not sure I understand the concept of plain decency. Can someone explain it to me?\", 'target': 'plain decency', 'ground_truth': ''}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "with open(\"./data/train/3.1_test1/raw_response_parse_iter3.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "# data\n",
    "transfer = {\n",
    "    '0': \"AGAINST\",\n",
    "    '1': \"FAVOR\",\n",
    "    '2': \"NONE\",\n",
    "    'FAVOR': \"FAVOR\",\n",
    "    'NONE': \"NONE\",\n",
    "    'AGAINST': \"AGAINST\"\n",
    "    \n",
    "}\n",
    "write_data = []\n",
    "\n",
    "for j in data:\n",
    "    # if len(i) == 10:\n",
    "        # for j in i:\n",
    "        try:\n",
    "            s = []\n",
    "            s.append(j[\"text\"])\n",
    "            s.append(j[\"target\"])\n",
    "            s.append(transfer[j[\"ground_truth\"].strip(\" \")])\n",
    "            s.append(1)\n",
    "            s.append(\"invalid\")\n",
    "            write_data.append(s)\n",
    "        except Exception as e:\n",
    "            print(j)\n",
    "len(write_data)\n",
    "csv_filename = './data/train/3.1_test1/raw_response_parse_iter3.csv'\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Tweet', 'Target 1', 'Stance 1', 'seen?', 'GT Target'])  # 写入标题行\n",
    "    writer.writerows(write_data)  # 写入数据行\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_filename = './data/all_error_instances_weakness_generate_train_dataset_2.csv'\n",
    "# import csv\n",
    "\n",
    "\n",
    "# # 打开CSV文件\n",
    "# with open(csv_filename, mode='r', encoding='utf-8') as file:\n",
    "#     # 创建CSV阅读器\n",
    "#     csv_reader = csv.reader(file)\n",
    "    \n",
    "#     # 遍历CSV文件中的每一行\n",
    "#     for row in csv_reader:\n",
    "#         print(row)  # 打印每一行，row是一个列表\n",
    "import json\n",
    "path = \"/home/ubuntu/wcc/now-task/scripts/results/baseline/checkpoint-318/log_train_parse_1_60_wrong_test_data.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "index = []\n",
    "for i in range(527):\n",
    "    index.append([0]*3)\n",
    "    for x in data:\n",
    "        if x[\"index\"] >= i*60 and x[\"index\"] < (i+1)*60:\n",
    "            index[i][(x[\"index\"]-i*60)//20] += 1\n",
    "ans = []\n",
    "for index_i,i in enumerate(index):\n",
    "    for index_j,j in enumerate(i):\n",
    "        if j == 20:\n",
    "            ans.append(index_i*60+index_j*20)\n",
    "print(len(ans))\n",
    "ans\n",
    "with open(\"/home/ubuntu/wcc/now-task/data/train/spurious_1_60_parse.json\",\"r\") as f:\n",
    "    parse = json.load(f)\n",
    "final_data = []\n",
    "for i in ans:\n",
    "    final_data += parse[i:i+20]\n",
    "\n",
    "with open(\"/home/ubuntu/wcc/now-task/data/train/train_spurious_1_60_parse_100%.json\",\"w\") as f:\n",
    "    json.dump(final_data,f,indent=4)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2]\n",
    "a[0:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Tweet  \\\n",
      "0      It's astonishing how some people blame their c...   \n",
      "1      The concept of personal accountability is cruc...   \n",
      "2      It's disappointing to see how some people lack...   \n",
      "3      Personal accountability is essential for build...   \n",
      "4      It's refreshing to see individuals who take pe...   \n",
      "...                                                  ...   \n",
      "57289  The United States of America is the most advan...   \n",
      "57290  The United States of America is the most advan...   \n",
      "57291  I'd just like to see the curriculum. Too many ...   \n",
      "57292  Analogies are a weak form of argument, but the...   \n",
      "57293  I personally believe that whatever the outcome...   \n",
      "\n",
      "                      Target 1 Stance 1  seen? GT Target  \n",
      "0              personal growth    FAVOR      1   invalid  \n",
      "1             self-improvement    FAVOR      1   invalid  \n",
      "2             entrepreneurship    FAVOR      1   invalid  \n",
      "3        relationship building    FAVOR      1   invalid  \n",
      "4      personal responsibility    FAVOR      1   invalid  \n",
      "...                        ...      ...    ...       ...  \n",
      "57289        intestate student     NONE      1   invalid  \n",
      "57290             public beach     NONE      1   invalid  \n",
      "57291              jail record     NONE      1   invalid  \n",
      "57292                 vacation     NONE      1   invalid  \n",
      "57293          mandatory child     NONE      1   invalid  \n",
      "\n",
      "[57294 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path1 = \"./data/train/3.1_test1/raw_response_parse_iter3.csv\"\n",
    "path2 = \"./data/train/3.1_test1/raw_response_parse_combined_iter2.csv\"\n",
    "# 读取两个 CSV 文件\n",
    "df1 = pd.read_csv(path1)\n",
    "df2 = pd.read_csv(path2)\n",
    "\n",
    "# 按行合并，自动对齐列\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 保存结果\n",
    "combined_df.to_csv(\"./data/train/3.1_test1/raw_response_parse_combined_iter3.csv\", index=False)\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n",
      "2345\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "path = \"/home/ubuntu/wcc/now-task/scripts/results/baseline/checkpoint-318/log_batch_request_wrong_test_data.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "y = []\n",
    "\n",
    "index = []\n",
    "for i in range(527):\n",
    "    index.append([0]*3)\n",
    "    for x in data:\n",
    "        if x[\"index\"] >= i*15 and x[\"index\"] < (i+1)*15:\n",
    "            index[i][(x[\"index\"]-i*15)//5] += 1\n",
    "# print(index)\n",
    "ans = []\n",
    "for index_i,i in enumerate(index):\n",
    "    for index_j,j in enumerate(i):\n",
    "        if j <= 3 and j >=1:\n",
    "            ans.append(index_i*15+index_j*5)\n",
    "print(len(ans))\n",
    "\n",
    "with open(\"/home/ubuntu/wcc/now-task/data/train/batch_request/raw_response_1_15_parse.json\", \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "d = []\n",
    "for i in ans:\n",
    "    for j in range(5):\n",
    "        d.append(raw_data[i+j])\n",
    "print(len(d))\n",
    "with open(\"/home/ubuntu/wcc/now-task/data/train/batch_request/raw_response_1_15_parse_20-60%.json\",\"w\")  as f:\n",
    "    json.dump(d,f,indent=4)\n",
    "    \n",
    "\n",
    "#     y.append(len(ans))\n",
    "# x = [_*5 for _ in range(6)]\n",
    "# print(len(x),len(y))\n",
    "# print(sum(y))\n",
    "# print(x)\n",
    "# print(y)\n",
    "# # 创建折线图\n",
    "# plt.plot(x, y, label=\"Prime Numbers\", color=\"blue\", marker=\"o\")\n",
    "# # 添加标题和标签\n",
    "# plt.title(\"前百分比错误率错误数\")\n",
    "# plt.xlabel(\"X-axis\")\n",
    "# plt.ylabel(\"Y-axis\")\n",
    "# # 显示图例\n",
    "# plt.legend()\n",
    "# # 显示图表\n",
    "# plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/ubuntu/wcc/now-task/data/train/train_spurious_1_60_parse_100%.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "s = [0]*3\n",
    "for i in data:\n",
    "    if i[\"ground_truth\"] == \"1\":\n",
    "        s[1] += 1\n",
    "    if i[\"ground_truth\"] == \"0\":\n",
    "        s[0] += 1\n",
    "    if i[\"ground_truth\"] == \"2\":\n",
    "        s[2] += 1\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/ubuntu/wcc/now-task/data/train/train_spurious_1_60_parse_combain_100%.csv\")\n",
    "s = [0]*3\n",
    "for _, d in data.iterrows():\n",
    "    if d[\"Stance 1\"] == \"NONE\":\n",
    "        s[1] += 1\n",
    "    if d[\"Stance 1\"] == \"FAVOR\":\n",
    "        s[0] += 1\n",
    "    if d[\"Stance 1\"] == \"AGAINST\":\n",
    "        s[2] += 1\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"/home/ubuntu/wcc/now-task/scripts/results/baseline/checkpoint-318/wrong_test_data.json\"\n",
    "path_50 = \"/home/ubuntu/wcc/now-task/scripts/results/spurious_1_60/checkpoint-342/log_testset_wrong_test_data.json\" \n",
    "path_90 = \"/home/ubuntu/wcc/now-task/scripts/results/spurious_1_60_90%/checkpoint-348/log_testset_wrong_test_data.json\"\n",
    "path_100 = \"/home/ubuntu/wcc/now-task/scripts/results/spurious_1_60_100%/checkpoint-696/log_testset_wrong_test_data.json\"\n",
    "import json\n",
    "def get_data(path):\n",
    "    with open(path,\"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "def get_same_data(baseline,compare):\n",
    "    b = get_data(baseline)\n",
    "    c = get_data(compare)\n",
    "    sum = 0\n",
    "    for i in b:\n",
    "        for j in c:\n",
    "            if i[\"index\"] == j[\"index\"]:\n",
    "                sum += 1\n",
    "    return sum/len(b)      \n",
    "print(len(get_data(baseline)))\n",
    "print(len(get_data(path_50)))\n",
    "print(len(get_data(path_90)))\n",
    "print(len(get_data(path_100)))\n",
    "\n",
    "print(\"val 错误率 50%\",get_same_data(baseline,path_50))\n",
    "print(\"val 错误率 90%\",get_same_data(baseline,path_90))\n",
    "print(\"val 错误率 100%\",get_same_data(baseline,path_100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"./scripts/results/baseline/checkpoint-318/log_dev_wrong_test_data.json\" # prompt给的数据\n",
    "response_error_path = \"/home/ubuntu/wcc/now-task/scripts/results/baseline/checkpoint-318/log_train_parse_1_60_wrong_test_data.json\" # baseline答错的数据\n",
    "response_raw_path= \"/home/ubuntu/wcc/now-task/data/train/spurious_1_60.json\" # 3部分\n",
    "index = []\n",
    "import json\n",
    "with open(raw_path,\"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "with open(response_raw_path,\"r\") as f:\n",
    "    raw_response = json.load(f)\n",
    "    \n",
    "with open(response_error_path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "def extract(data,num):\n",
    "    text = data.strip(\"```xml\").strip(\"```\").split(\"\\n\\n\")\n",
    "    return text[num]      \n",
    "for i in range(527):\n",
    "    index.append([0]*3)\n",
    "    for x in data:\n",
    "        if x[\"index\"] >= i*60 and x[\"index\"] < (i+1)*60:\n",
    "            index[i][(x[\"index\"]-i*60)//20] += 1\n",
    "ans = []\n",
    "final_data = []\n",
    "for index_i,i in enumerate(index):\n",
    "    x = {}\n",
    "    x[\"raw_question\"] = raw_data[index_i][\"input_text\"]\n",
    "    x[\"raw_target\"] = raw_data[index_i][\"target\"]\n",
    "    x[\"raw_true_label\"] = raw_data[index_i][\"true_label\"]\n",
    "    x[\"raw_predict_label\"] = raw_data[index_i][\"predicted_label\"]\n",
    "    x[\"pattern\"] = \"\"\n",
    "    for index_j,j in enumerate(i):\n",
    "        if j >= 20: # 控制百分比\n",
    "            x[\"pattern\"] += extract(raw_response[index_i],index_j)\n",
    "            \n",
    "    if x[\"pattern\"] != \"\":\n",
    "        # x[\"pattern\"] = json.load(x[\"pattern\"])\n",
    "        final_data.append(x)\n",
    "final_data\n",
    "with open(\"spurious_1_60_100%_with_raw_question_and_pattern_data.json\",\"w\") as f:\n",
    "    json.dump(final_data,f, ensure_ascii=False,indent=4)            \n",
    "            \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"python eval.py --model_root_path '1' --test_path '2' --output_error_question False --save_log True --sign '3'\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "current_results_path = 1\n",
    "test_path = 2\n",
    "sign = 3\n",
    "ls.append(\n",
    "            f\"python eval.py \"\n",
    "            f\"--model_root_path '{current_results_path}' \"\n",
    "            f\"--test_path '{test_path}' \"\n",
    "            f\"--output_error_question False \"\n",
    "            f\"--save_log True \"\n",
    "            f\"--sign '{sign}'\"\n",
    ")\n",
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_file_structure(path):\n",
    "    \"\"\"递归遍历文件夹，生成文件结构\"\"\"\n",
    "    structure = []\n",
    "    \n",
    "    # 遍历文件和文件夹\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # 计算当前目录的深度\n",
    "        depth = root.replace(path, '').count(os.sep)\n",
    "        indent = '│   ' * depth  # 每级目录缩进4个字符\n",
    "        folder_name = os.path.basename(root)\n",
    "        structure.append(f\"{indent}├── {folder_name}/\" if folder_name else '')\n",
    "        \n",
    "        # 添加文件\n",
    "        for file in files:\n",
    "            structure.append(f\"{indent}│   └── {file}\")  # 文件的缩进方式\n",
    "    return structure\n",
    "\n",
    "def write_to_readme(file_structure, output_file='README.md'):\n",
    "    \"\"\"将文件结构写入 README.md\"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# 项目文件结构\\n\\n\")\n",
    "        f.write(\"```plaintext\\n\")\n",
    "        f.write(\"\\n\".join(file_structure))\n",
    "        f.write(\"\\n```\\n\")\n",
    "\n",
    "# 使用时指定项目路径\n",
    "project_path = '.'  # 当前路径，也可以修改为你的项目根目录路径\n",
    "file_structure = generate_file_structure(project_path)\n",
    "write_to_readme(file_structure)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wcc_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
