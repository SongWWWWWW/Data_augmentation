{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import json\n",
    "def parse(answer:str,args):\n",
    "    pattern = r\"<text>(.*?)</text>\\s*<target>(.*?)</target>\\s*<ground_truth>(.*?)</ground_truth>\"\n",
    "    matches = re.findall(pattern, answer)\n",
    "    ans = []\n",
    "    for match in matches:\n",
    "        ans.append({\n",
    "            \"text\":match[0],\n",
    "            \"target\":match[1],\n",
    "            \"ground_truth\":match[2]\n",
    "        })\n",
    "    if len(ans) != args.spurious_num*args.generate_num:\n",
    "        return []\n",
    "    return ans\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.spurious_num = 3\n",
    "        self.generate_num = 10\n",
    "path = \"/home/ubuntu/wcc/now-task/data/train/prompt2_3.1_7_3_0.8/iter3_raw_response.json\"\n",
    "with open(path,\"r\") as f:\n",
    "    data = json.load(f)\n",
    "args = Args()\n",
    "parse_data = []\n",
    "for i in data:\n",
    "    parse_data += parse(i,args)\n",
    "print(parse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from response import get_batch_response\n",
    "import json\n",
    "import re\n",
    "model = \"meta.llama3-1-8b-instruct-v1:0\"\n",
    "\n",
    "def format_prompts(data):\n",
    "    # prompt = \"\"\"\n",
    "    # I am training a model using RoBERTa + MLP on a task named {task_name}. The task involves {task_description}. \n",
    "    # Your task is to identify potential spurious patterns that the model might have learned based on its responses.\n",
    "\n",
    "    # I will present you with an instance where the model provided incorrect responses. \n",
    "\n",
    "    # Please provide {spurious_num} assumptions of spurious patterns that may have caused the incorrect response. And you should provide\n",
    "    # how to generate corresponding training data about every spurious pattern in those patterns.\n",
    "    \n",
    "    # A spurious pattern refers to a misleading or non-causal feature relationship that the model learns during training, \n",
    "    # such as misunderstandings of certain phrases, sentiment words, or entity relations. Be specific in the patterns, such as what words or what relations, but not a general description.\n",
    "\n",
    "    # Format your evaluation instances using XML tags <Spurious_i> and </Spurious_i>. \n",
    "\n",
    "    # The incorrect instance is as follows:\n",
    "    # {data}\n",
    "    # Please output all content completely without omitting or summarizing.\n",
    "\n",
    "    # \"\"\"\n",
    "    prompt = \"\"\"\n",
    "    I am training a model using RoBERTa + MLP on a task named {task_name}. The task involves {task_description}. \n",
    "    Your task is to identify potential spurious patterns that the model might have learned based on its responses.\n",
    "\n",
    "    I will present you with an instance where the model provided incorrect responses.\n",
    "\n",
    "    Please provide {spurious_num} assumptions of spurious patterns that may have caused the incorrect response. For each spurious pattern, also provide a detailed strategy for generating corresponding training data to test or mitigate the identified spurious pattern.\n",
    "    \n",
    "    The “generate strategy” should induce the model to increase the diversity of the generated data as much as possible.\n",
    "\n",
    "    A spurious pattern refers to a misleading or non-causal feature relationship that the model learns during training, such as misunderstandings of certain phrases, sentiment words, or entity relations. Be specific in the patterns, such as what words or what relations, and avoid general descriptions.\n",
    "\n",
    "    The incorrect instance is as follows:\n",
    "    {data}\n",
    "    Please ensure the output is formatted as follows:\n",
    "\n",
    "    ```xml\n",
    "    <SpuriousPatterns>\n",
    "        <Spurious_1>\n",
    "            <Pattern>Description of spurious pattern 1 (specific and detailed)</Pattern>\n",
    "            <GenerateStrategy>Detailed strategy to generate training data for spurious pattern 1</GenerateStrategy>\n",
    "        </Spurious_1>\n",
    "        <Spurious_2>\n",
    "            <Pattern>Description of spurious pattern 2 (specific and detailed)</Pattern>\n",
    "            <GenerateStrategy>Detailed strategy to generate training data for spurious pattern 2</GenerateStrategy>\n",
    "        </Spurious_2>\n",
    "        ...\n",
    "    </SpuriousPatterns>\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    task_name = \"stance detection\"\n",
    "    task_description = \"Stance detection aims to identify the authors' attitudes or positions [FAVOR, NONE, AGAINST] towards a specific target such as an entity, a topic.\"\n",
    "    spurious_num = 3\n",
    "    # generate_num = \n",
    "    return prompt.format(task_name=task_name,task_description=task_description,data=data,spurious_num=spurious_num)\n",
    "def parse(s:str):\n",
    "    pattern_regex = re.compile(r\"<Pattern>(.*?)</Pattern>\\s*<GenerateStrategy>(.*?)</GenerateStrategy>\",re.DOTALL)\n",
    "\n",
    "    # 匹配内容\n",
    "    matches = pattern_regex.findall(s)\n",
    "    ans = []\n",
    "    # 提取结果\n",
    "    for i, (pattern, generate_strategy) in enumerate(matches, 1):\n",
    "        # print(f\"Spurious_{i}:\")\n",
    "        # print(f\"Pattern: {pattern.strip()}\")\n",
    "        # print(f\"GenerateStrategy: {generate_strategy.strip()}\")\n",
    "        # print(\"-\" * 50)\n",
    "        ans.append({\n",
    "            \"Pattern\":pattern.strip(),\n",
    "            \"GenerateStrategy\": generate_strategy.strip()\n",
    "        })\n",
    "    return ans\n",
    "def parse2(s:str):\n",
    "    pattern = r\"<text>(.*?)</text>\\s*<target>(.*?)</target>\\s*<ground_truth>(.*?)</ground_truth>\"\n",
    "    matches = re.findall(pattern, s)\n",
    "    ans = []\n",
    "    for match in matches:\n",
    "        ans.append({\n",
    "            \"text\":match[0],\n",
    "            \"target\":match[1],\n",
    "            \"ground_truth\":match[2]\n",
    "        })\n",
    "    # if len(ans) != args.spurious_num*args.generate_num:\n",
    "    #     return []\n",
    "    return ans\n",
    "def format_output_prompt(spurious,generate_strategy):\n",
    "    prompt = \"\"\"\n",
    "    \n",
    "    I am training a model using RoBERTa + MLP on a task named {task_name}. The task involves {task_description}. \n",
    "    Your task is to generate diverse and contextually appropriate training data based on the provided spurious pattern and generation strategy.\n",
    "\n",
    "    The spurious pattern and corresponding generation strategy are as follows:\n",
    "    <Spurious>\n",
    "        <Pattern>{spurious}</Pattern>\n",
    "        <GenerateStrategy>{generate_strategy}</GenerateStrategy>\n",
    "    </Spurious>\n",
    "\n",
    "    Based on the provided spurious pattern and generation strategy, generate {generate_num} verification data points. \n",
    "    Each generated data point should align with the spurious pattern and adhere to the specified generation strategy.\n",
    "\n",
    "    ### Output Format:\n",
    "    Each verification data point should be structured as follows:\n",
    "    - <verification_i> for each data point, where `i` is the sequential number of the verification set.\n",
    "\n",
    "    Each <verification_i> should contain the following:\n",
    "    1. <text>: A multi-sentence passage (at least 100 words) containing the spurious pattern within a suitable context. \n",
    "    The passage should demonstrate the identified spurious pattern while maintaining coherence and diversity.\n",
    "    2. <target>: An entity or phrase from the text that is the focus of the classification task.\n",
    "    3. <ground_truth>: The true label for the classification task, ensuring logical consistency with the provided text, you can just use one of [\"FAVOR\",\"AGAINST\",\"NONE\"].\n",
    "\n",
    "    Ensure that the generated data points are diverse, use various speaking styles, and include different entities and contexts to avoid overfitting during model fine-tuning.\n",
    "\n",
    "    ### Example Output:\n",
    "    <verification_1>\n",
    "        <text>\"She always goes the extra mile to assist her colleagues and solve problems effectively.\"</text>\n",
    "        <target>\"colleagues\"</target>\n",
    "        <ground_truth>FAVOR</ground_truth>\n",
    "    </verification_1>\n",
    "    <verification_2>\n",
    "        <text>\"The weather always changes unpredictably in this region, making planning difficult.\"</text>\n",
    "        <target>\"weather\"</target>\n",
    "        <ground_truth>NONE</ground_truth>\n",
    "    </verification_2>\n",
    "    <verification_3>\n",
    "        <text>\"He always delays submitting his reports, which causes unnecessary delays in the project.\"</text>\n",
    "        <target>\"reports\"</target>\n",
    "        <ground_truth>AGAINST</ground_truth>\n",
    "    </verification_3>\n",
    "\n",
    "    ### Now, generate the output based on the following inputs:\n",
    "    <Spurious>\n",
    "        <Pattern>{spurious}</Pattern>\n",
    "        <GenerateStrategy>{generate_strategy}</GenerateStrategy>\n",
    "    </Spurious>\n",
    "\n",
    "    \"\"\"\n",
    "    task_name = \"stance detection\"\n",
    "    task_description = \"Stance detection aims to identify the authors' attitudes or positions [FAVOR, NONE, AGAINST] towards a specific target such as an entity, a topic.\"\n",
    "    generate_num = 10\n",
    "    return prompt.format(task_name=task_name,task_description=task_description,generate_num=generate_num,spurious=spurious,generate_strategy=generate_strategy)\n",
    "\n",
    "with open(\"./results/baseline/checkpoint-318/log_dev_wrong_test_data.json\",\"r\") as f:\n",
    "    data = json.load(f)\n",
    "prompts = []\n",
    "for i in range(10):\n",
    "    prompts.append(format_prompts(data=data[i]))            \n",
    "# print(get_batch_response(model,[prompt])[0])\n",
    "print(len(prompts))\n",
    "answer = get_batch_response(model,prompts)\n",
    "\n",
    "patterns = []\n",
    "for i in answer:\n",
    "    # print(i)\n",
    "    # print(\"--------\")\n",
    "    patterns.extend(parse(i))\n",
    "\n",
    "prompt2 = []\n",
    "for i in range(10):\n",
    "    prompt2.append(format_output_prompt(patterns[i][\"Pattern\"],patterns[i][\"GenerateStrategy\"]))\n",
    "    \n",
    "answer2 = get_batch_response(model,prompt2)\n",
    "parse_output = []\n",
    "for i in answer2:\n",
    "    # print(i)\n",
    "    parse_output.extend(parse2(i))\n",
    "print(parse_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/strategy1_3.1_7_3_0.0_iter1/checkpoint-768']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def get_model_path(path:str):\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # print(dirs)\n",
    "        dirs.sort()\n",
    "        for dir_name in dirs:\n",
    "            \n",
    "            if dir_name == \"logs\":\n",
    "                continue\n",
    "            # print(dirs)\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            # print(\"Folder:\", dir_path)            \n",
    "            paths.append(dir_path)\n",
    "    return paths\n",
    "get_model_path('../results/strategy1_3.1_7_3_0.0_iter1/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_augmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
